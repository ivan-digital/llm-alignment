# Experiment Results Log

## Experiment 1: Catastrophic Forgetting (FAILED ‚ùå)
**Date:** 2025-08-05 to 2025-08-07 | **Duration:** 47 hours

### Parameters
| Parameter | Value | Issue |
|-----------|--------|-------|
| Learning Rate | 5e-5 | Too high ‚ùå |
| LoRA Rank | 16 | Too aggressive ‚ùå |
| LoRA Alpha | 32 | Too aggressive ‚ùå |
| Early Stopping | None | Missing ‚ùå |
| Regularization | None | Missing ‚ùå |
| Generation Config | Default | Poor quality ‚ùå |

### Results
| Metric | Baseline | Final | Delta | Status |
|--------|----------|-------|-------|--------|
| T-Wix Loss | 1.2848 | 0.8430 | -34.4% | ‚úÖ Good |
| T-Wix Perplexity | 3.6141 | 2.3234 | -35.7% | ‚úÖ Good |
| MT-Bench Average | 2.0357 | 1.4924 | **-26.7%** | ‚ùå **Catastrophic Forgetting** |

**Root Cause:** Aggressive parameters caused severe overfitting - model learned training data but forgot general capabilities.

---

## Experiment 2: Conservative Training (PARTIAL SUCCESS ‚ö†Ô∏è)
**Date:** 2025-08-08 to 2025-08-10 | **Duration:** 47 hours | **Dataset:** Full (100% - sampling bug)

### Parameters Used
| Parameter | Planned | Actual | Issue |
|-----------|---------|--------|-------|
| Learning Rate | 1e-5 | ‚úÖ 1e-5 | Correct |
| LoRA Rank | 8 | ‚úÖ 8 | Correct |
| LoRA Alpha | 16 | ‚úÖ 16 | Correct |
| Early Stopping | 3 patience | ‚ùå Not triggered | No MT-Bench during training |
| Sample Ratio | 25% | ‚ùå 100% | **Sampling bug - used full dataset** |
| Regularization | ‚úÖ Weight decay, grad clipping | Applied |

### Results
| Metric | Baseline | Final | Delta | Status |
|--------|----------|-------|-------|--------|
| T-Wix Loss | 1.2848 | 0.9043 | -29.6% | ‚úÖ Good improvement |
| T-Wix Perplexity | 3.6141 | 2.4701 | -31.6% | ‚úÖ Good improvement |
| MT-Bench Average | **2.6565** | **2.4430** | **-8.0%** | ‚ö†Ô∏è **Mild Forgetting** |

### Analysis
‚úÖ **Better than Exp 1:** MT-Bench degradation reduced from -26.7% to -8.0%  
‚úÖ **Conservative params worked:** Less aggressive overfitting  
‚ùå **Still some forgetting:** Many questions dropped to 1.0 (minimum score)  
‚ùå **Sampling failed:** Used full dataset instead of 25% (47 hours vs planned 3 hours)  
‚ùå **Early stopping unused:** MT-Bench evaluation not triggered during training  

---

## Experiment 3: Fast Sampling (READY TO TEST üîÑ)

### Fixed Parameters for Next Run
| Parameter | Value | Reason |
|-----------|-------|---------|
| Sample Ratio | **0.25** (fix default) | 4x faster training |
| Early Stopping | **Enable during training** | Catch forgetting early |
| Eval Steps | **100** | More frequent MT-Bench checks |
| Other Params | Same as Exp 2 | Keep conservative settings |

**Command to run:**
```bash
poetry run python sft_train.py --learning_rate 1e-5 --lora_r 8 --lora_alpha 16 --early_stop_patience 3 --eval_steps 100
```
(Should now use 25% sampling by default)