{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e08ccbf",
   "metadata": {},
   "source": [
    "# Qwen adapters: Soft Prompt → LoRA SFT → KL‑anchored SFT \n",
    "\n",
    "This workshop notebook uses the open, commercially usable Databricks Dolly 15k dataset (`databricks/databricks-dolly-15k`).\n",
    "\n",
    "Run order: 1) Setup → 2) Data → 3) Template → 4) KL toy → 5) Soft Prompt → 6) LoRA SFT → 7) KL‑anchored SFT → 8) Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38072b6",
   "metadata": {},
   "source": [
    "\n",
    "### Workshop Goals\n",
    "- Show minimal, runnable examples of three adapter techniques: Soft Prompt token tuning, LoRA SFT, and KL-anchored SFT on an open dataset (Dolly 15k).\n",
    "- Emphasize how to preprocess with chat templates, why/when to use Soft Prompt vs LoRA, and how a small KL penalty anchors behavior to the base model.\n",
    "- Keep compute small: tiny instruct model, small sample sizes, short training with frequent eval to illustrate loss going down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4912a74a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:17.781615Z",
     "iopub.status.busy": "2025-10-26T10:26:17.781615Z",
     "iopub.status.idle": "2025-10-26T10:26:19.375128Z",
     "shell.execute_reply": "2025-10-26T10:26:19.374551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cpu\n",
      "CUDA available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# If running on a fresh environment, you may need:\n",
    "# !pip install -U transformers accelerate datasets trl peft bitsandbytes einops pandas safetensors\n",
    "\n",
    "import os, math, json, random, logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Global switches\n",
    "USE_4BIT  = False   # Enable if bitsandbytes is available and you have a GPU\n",
    "SEED      = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Choose a small instruct model to keep things light.\n",
    "# You can switch to Qwen3 if you have a larger GPU (e.g., 'Qwen/Qwen3-1.7B-Instruct').\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0146a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:19.376701Z",
     "iopub.status.busy": "2025-10-26T10:26:19.376174Z",
     "iopub.status.idle": "2025-10-26T10:26:21.151056Z",
     "shell.execute_reply": "2025-10-26T10:26:21.150526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Dolly 15k and render to `text` using the tokenizer's chat template.\n",
    "# We keep only the `text` field to be consumed by the trainer/tokenizer.\n",
    "from typing import Optional, List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def load_tokenizer():\n",
    "    return AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "def load_base_model():\n",
    "    if USE_4BIT:\n",
    "        bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID, trust_remote_code=True, quantization_config=bnb, device_map=\"auto\"\n",
    "        )\n",
    "    return AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "def to_chatml(messages: List[Dict[str,str]], tok):\n",
    "    return tok.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "def build_messages(instruction: str, context: str, response: str) -> List[Dict[str,str]]:\n",
    "    sys = \"You are a helpful assistant.\"\n",
    "    if context:\n",
    "        user = f\"Instruction: {instruction}\\nContext: {context}\"\n",
    "    else:\n",
    "        user = f\"Instruction: {instruction}\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "        {\"role\": \"assistant\", \"content\": (response or \"\")}\n",
    "    ]\n",
    "\n",
    "def load_dolly_as_sft(tok, max_samples: Optional[int] = None) -> Dataset:\n",
    "    ds = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    if max_samples is not None:\n",
    "        ds = ds.select(range(min(int(max_samples), len(ds))))\n",
    "\n",
    "    def to_text(ex):\n",
    "        msgs = build_messages(ex.get(\"instruction\", \"\"), ex.get(\"context\", \"\"), ex.get(\"response\", \"\"))\n",
    "        return {\"text\": to_chatml(msgs, tok)}\n",
    "\n",
    "    ds = ds.map(to_text, remove_columns=ds.column_names)\n",
    "    return ds\n",
    "\n",
    "import random as _rnd\n",
    "def split_small(ds, test_size=0.1, seed=42):\n",
    "    n = len(ds)\n",
    "    idx = list(range(n))\n",
    "    _rnd.Random(seed).shuffle(idx)\n",
    "    cut = max(1, int(n * (1 - test_size)))\n",
    "    train_idx, test_idx = idx[:cut], idx[cut:]\n",
    "    train_texts = [ds[i]['text'] for i in train_idx]\n",
    "    test_texts  = [ds[i]['text'] for i in test_idx]\n",
    "    from datasets import Dataset as _DS\n",
    "    return _DS.from_dict({'text': train_texts}), _DS.from_dict({'text': test_texts})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd8a5d",
   "metadata": {},
   "source": [
    "## 1) Data — Databricks Dolly 15k (open source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ad8e6",
   "metadata": {},
   "source": [
    "#### Dataset: Databricks Dolly 15k\n",
    "- Open, instruction-following dataset. We use a small subset to keep runs fast.\n",
    "- Mapping: we render each (instruction, optional context, response) as chat messages and apply the model's chat template to produce `text`.\n",
    "- Always preprocess with `apply_chat_template` to keep training/inference consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08390f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:21.152613Z",
     "iopub.status.busy": "2025-10-26T10:26:21.152613Z",
     "iopub.status.idle": "2025-10-26T10:26:25.822379Z",
     "shell.execute_reply": "2025-10-26T10:26:25.821864Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 483.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Instruction: When did Virgin Australia start operating?\n",
      "Context: Virgin Australia, the trading name of Virgin Australia Airli\n",
      "---\n",
      "Example 2\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Instruction: Which is a species of fish? Tope or Rope<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Tope<|im_end|>\n",
      "\n",
      "---\n",
      "Example 3\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Instruction: Why can camels survive for long without water?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Camels use the fat in their humps\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok = load_tokenizer()\n",
    "preview = load_dolly_as_sft(tok, max_samples=3)\n",
    "for i, t in enumerate(preview['text']):\n",
    "    print(f'Example {i+1}')\n",
    "    print(t[:200])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb40d1c",
   "metadata": {},
   "source": [
    "## 2) Template practice — render chat with the tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3fe5f",
   "metadata": {},
   "source": [
    "#### Chat Template\n",
    "- The tokenizer's chat template inserts system/user/assistant markers and special tokens.\n",
    "- For supervised fine-tuning, include the assistant answer in `text` so the loss is computed on assistant targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03209e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:25.823440Z",
     "iopub.status.busy": "2025-10-26T10:26:25.823440Z",
     "iopub.status.idle": "2025-10-26T10:26:26.535570Z",
     "shell.execute_reply": "2025-10-26T10:26:26.535040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give three bullet tips for staying focused when studying.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "1) Set a clear goal. 2) Use short sprints. 3) Remove distractions.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = load_tokenizer()\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Give three bullet tips for staying focused when studying.\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"1) Set a clear goal. 2) Use short sprints. 3) Remove distractions.\"},\n",
    "]\n",
    "rendered = tok.apply_chat_template(messages, tokenize=False)\n",
    "print(rendered[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee257f1b",
   "metadata": {},
   "source": [
    "## 3) KL divergence — tiny 3–5 token example\n",
    "We estimate token‑wise KL as KL(P‖Q) = ∑ p_i · (log p_i − log q_i).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb80f35",
   "metadata": {},
   "source": [
    "### KL Divergence - Intuition\n",
    "- Measures how one token distribution (student) diverges from a reference (base).\n",
    "- For next-token training, we add a small KL term per position: KL(P||Q) = sum p_i * (log p_i - log q_i).\n",
    "- Keeping KL small helps retain base model behavior while adapting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f39f4",
   "metadata": {},
   "source": [
    "#### KL Toy Example Notes\n",
    "- We compute entropy `H(P)`, cross-entropy `H(P,Q)`, and KL `KL(P||Q)` and print per-token contributions.\n",
    "- Positive `KL_i` means the student over-weights a token vs reference; negative means under-weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc14c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:26.536611Z",
     "iopub.status.busy": "2025-10-26T10:26:26.536611Z",
     "iopub.status.idle": "2025-10-26T10:26:26.541730Z",
     "shell.execute_reply": "2025-10-26T10:26:26.541216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token  p        q        -p*log p  -p*log q   KL_i\n",
      "    A  0.700000  0.600000  0.249672  0.357578  0.107905\n",
      "    B  0.100000  0.200000  0.230259  0.160944  -0.069315\n",
      "    C  0.150000  0.150000  0.284568  0.284568  0.000000\n",
      "    D  0.050000  0.050000  0.149787  0.149787  0.000000\n",
      "H(P)=0.914286, H(P,Q)=0.952876, KL(P||Q)=0.038591\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokens = ['A','B','C','D']\n",
    "# Two categorical distributions over the same 4 tokens\n",
    "p = torch.tensor([0.70, 0.10, 0.15, 0.05], dtype=torch.float64)  # student\n",
    "q = torch.tensor([0.60, 0.20, 0.15, 0.05], dtype=torch.float64)  # reference\n",
    "\n",
    "def dkl(p, q):\n",
    "    return torch.sum(p * (torch.log(p) - torch.log(q)))\n",
    "\n",
    "# Per-token contributions\n",
    "logp = torch.log(p)\n",
    "logq = torch.log(q)\n",
    "H_p_i  = -p * logp\n",
    "H_pq_i = -p * logq\n",
    "KL_i   = p * (logp - logq)\n",
    "\n",
    "H_p  = H_p_i.sum().item()\n",
    "H_pq = H_pq_i.sum().item()\n",
    "KL   = KL_i.sum().item()\n",
    "\n",
    "print('Token  p        q        -p*log p  -p*log q   KL_i')\n",
    "for i, t in enumerate(tokens):\n",
    "    print('{:>5}  {:.6f}  {:.6f}  {:.6f}  {:.6f}  {:.6f}'.format(t, p[i].item(), q[i].item(), H_p_i[i].item(), H_pq_i[i].item(), KL_i[i].item()))\n",
    "\n",
    "print('H(P)={:.6f}, H(P,Q)={:.6f}, KL(P||Q)={:.6f}'.format(H_p, H_pq, KL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc7a35",
   "metadata": {},
   "source": [
    "## 4) Soft prompt tuning (Prompt Tuning) — no base weight updates\n",
    "We train a small set of virtual tokens. Start with few steps/epochs for speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f59b50",
   "metadata": {},
   "source": [
    "### Soft Prompt Token Tuning — What and Why\n",
    "- Learns a small set of virtual tokens prepended to every input; base weights stay frozen.\n",
    "- Good for steering style/format quickly with tiny memory and training cost.\n",
    "- We initialize with a short instruction text and optimize the prompt embeddings.\n",
    "- Below we show pre-train loss/perplexity, then train with eval every few steps to show the curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95acba",
   "metadata": {},
   "source": [
    "### Soft Prompt Token Tuning (Prompt Tuning)\n",
    "- Learns `num_virtual_tokens` embeddings prepended to every input; base model weights are frozen.\n",
    "- Great for style/format steering or small task bias with minimal memory/latency overhead.\n",
    "- Key knobs: `num_virtual_tokens` (capacity), LR (~5e-3 typical), sequence length.\n",
    "- We evaluate every few steps to show the validation loss curve trending down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c3f2060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:26.542795Z",
     "iopub.status.busy": "2025-10-26T10:26:26.542795Z",
     "iopub.status.idle": "2025-10-26T10:26:43.668605Z",
     "shell.execute_reply": "2025-10-26T10:26:43.668050Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 3967.68 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6995.69 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 1701.20 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft-prompt pre-train eval_loss: 2.9554014205932617 ppl: 19.20943223403436\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.527400</td>\n",
       "      <td>2.712756</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.661100</td>\n",
       "      <td>2.620221</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.389800</td>\n",
       "      <td>2.559868</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.419900</td>\n",
       "      <td>2.547457</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft-prompt post-train eval_loss: 2.547456979751587 ppl: 12.774576434801709\n",
      "Eval losses over steps: [(5, 2.712756395339966), (10, 2.620220899581909), (15, 2.559868097305298), (20, 2.547456979751587), (20, 2.547456979751587)]\n"
     ]
    }
   ],
   "source": [
    "# Configure Soft Prompt: number of virtual tokens, init from a short text,\n",
    "# and ensure task type matches causal LM.\n",
    "from peft import PromptTuningConfig, PromptTuningInit, TaskType, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "tok = load_tokenizer()\n",
    "base = load_base_model()\n",
    "\n",
    "ds = load_dolly_as_sft(tok, max_samples=60)\n",
    "train_ds, val_ds = split_small(ds, test_size=0.1, seed=SEED)\n",
    "\n",
    "peft_cfg = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=32,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text='You are a helpful assistant.',\n",
    "    tokenizer_name_or_path=MODEL_ID,\n",
    ")\n",
    "soft_model = get_peft_model(base, peft_cfg)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    enc = tok(batch['text'], truncation=True, max_length=256)\n",
    "    # labels created by collator\n",
    "    return enc\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='out_soft_prompt',\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-3,\n",
    "    logging_steps=5,\n",
    "    eval_steps=5,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='no',\n",
    "    report_to=[], max_steps=20,\n",
    "    fp16=False,\n",
    ")\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "trainer = Trainer(model=soft_model, args=args, train_dataset=train_tok, eval_dataset=val_tok, data_collator=collator)\n",
    "pre = trainer.evaluate()\n",
    "pre_ppl = math.exp(pre['eval_loss']) if pre['eval_loss'] < 20 else float('inf')\n",
    "print('Soft-prompt pre-train eval_loss:', pre['eval_loss'], 'ppl:', pre_ppl)\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "ppl = math.exp(metrics['eval_loss']) if metrics['eval_loss'] < 20 else float('inf')\n",
    "print('Soft-prompt post-train eval_loss:', metrics['eval_loss'], 'ppl:', ppl)\n",
    "print('Eval losses over steps:', [(h.get('step'), h.get('eval_loss')) for h in trainer.state.log_history if 'eval_loss' in h])\n",
    "trainer.save_model('out_soft_prompt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f657ac",
   "metadata": {},
   "source": [
    "## 5) LoRA SFT — parameter‑efficient fine‑tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45008581",
   "metadata": {},
   "source": [
    "### LoRA SFT\n",
    "- Inserts low-rank adapters on attention projections (here q_proj/v_proj).\n",
    "- Key knobs: rank `r`, `lora_alpha` (scale), `lora_dropout`.\n",
    "- Enables learning beyond prompt tokens while remaining parameter-efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c1861d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:26:43.670162Z",
     "iopub.status.busy": "2025-10-26T10:26:43.670162Z",
     "iopub.status.idle": "2025-10-26T10:27:08.135754Z",
     "shell.execute_reply": "2025-10-26T10:27:08.135225Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 7102.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/108 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 108/108 [00:00<00:00, 11582.62 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 4322.91 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA pre-train eval_loss: 2.8462913036346436 ppl: 17.223785451830402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:16, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.982700</td>\n",
       "      <td>2.819744</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.656800</td>\n",
       "      <td>2.800092</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.621000</td>\n",
       "      <td>2.787484</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.646600</td>\n",
       "      <td>2.782698</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA post-train eval_loss: 2.7826976776123047 ppl: 16.162563575550376\n",
      "Eval losses over steps: [(5, 2.8197438716888428), (10, 2.8000917434692383), (15, 2.7874844074249268), (20, 2.7826976776123047), (20, 2.7826976776123047)]\n"
     ]
    }
   ],
   "source": [
    "# LoRA on attention projections: r (rank), alpha (scale), dropout (regularization).\n",
    "# We enable gradient checkpointing and input grads for stability with small VRAM.\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "tok = load_tokenizer()\n",
    "base = load_base_model()\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias='none', task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj','v_proj']\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "ds = load_dolly_as_sft(tok, max_samples=120)\n",
    "train_ds, val_ds = split_small(ds, test_size=0.1, seed=SEED)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    enc = tok(batch['text'], truncation=True, max_length=256)\n",
    "    # labels created by collator\n",
    "    return enc\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='out_lora_sft',\n",
    "    per_device_train_batch_size=2, gradient_accumulation_steps=1, num_train_epochs=1,\n",
    "    learning_rate=1e-4, fp16=False, logging_steps=5,\n",
    "    eval_steps=5, eval_strategy='steps', save_strategy='no', report_to=[], max_steps=20,\n",
    ")\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_tok, eval_dataset=val_tok, data_collator=collator)\n",
    "pre = trainer.evaluate()\n",
    "pre_ppl = math.exp(pre['eval_loss']) if pre['eval_loss'] < 20 else float('inf')\n",
    "print('LoRA pre-train eval_loss:', pre['eval_loss'], 'ppl:', pre_ppl)\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "ppl = math.exp(metrics['eval_loss']) if metrics['eval_loss'] < 20 else float('inf')\n",
    "print('LoRA post-train eval_loss:', metrics['eval_loss'], 'ppl:', ppl)\n",
    "print('Eval losses over steps:', [(h.get('step'), h.get('eval_loss')) for h in trainer.state.log_history if 'eval_loss' in h])\n",
    "trainer.save_model('out_lora_sft')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c198794",
   "metadata": {},
   "source": [
    "## 6) KL‑anchored SFT — keep adapter near the base\n",
    "Add a small token‑wise KL term between the LoRA student and a frozen base reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0ddf4",
   "metadata": {},
   "source": [
    "### KL-Anchored SFT\n",
    "- Adds a small token-wise KL penalty between the LoRA student and a frozen base reference.\n",
    "- Warm up `kl_coef` over a few hundred steps, optionally limit to tail tokens, and mask padding with `-100`.\n",
    "- Helps retain base capabilities while adapting to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01fc2b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:27:08.137330Z",
     "iopub.status.busy": "2025-10-26T10:27:08.137330Z",
     "iopub.status.idle": "2025-10-26T10:27:37.349497Z",
     "shell.execute_reply": "2025-10-26T10:27:37.349497Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 6363.68 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/108 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 108/108 [00:00<00:00, 9763.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 4604.91 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL-SFT pre-train eval_loss: 2.874230146408081 ppl: 17.711783391389677\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.016500</td>\n",
       "      <td>2.847374</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.686200</td>\n",
       "      <td>2.827473</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.647100</td>\n",
       "      <td>2.814700</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.668000</td>\n",
       "      <td>2.809862</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL-SFT post-train eval_loss: 2.8098623752593994 ppl: 16.60763244039388\n",
      "Eval losses over steps: [(5, 2.847374200820923), (10, 2.8274734020233154), (15, 2.8147003650665283), (20, 2.8098623752593994), (20, 2.8098623752593994)]\n"
     ]
    }
   ],
   "source": [
    "# Wrap HF Trainer to inject a token-wise KL term vs a frozen reference model.\n",
    "# We compute next-token KL by shifting logits, mask out ignored labels, and average per token.\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "class KLTrainer(Trainer):\n",
    "    def __init__(self, *args, ref_model=None, kl_coef=0.05, kl_warmup_steps=300, kl_limit_seq=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ref_model = ref_model\n",
    "        self.kl_coef = float(kl_coef)\n",
    "        self.kl_warmup_steps = int(max(1, kl_warmup_steps))\n",
    "        self.kl_limit_seq = int(max(0, kl_limit_seq))\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if self.ref_model is not None and self.kl_coef > 0 and model.training:\n",
    "            step = int(getattr(self.state, 'global_step', 0) or 0)\n",
    "            coef = self.kl_coef * min(1.0, step / float(self.kl_warmup_steps))\n",
    "            with torch.no_grad():\n",
    "                ref = self.ref_model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask'))\n",
    "            logits_s = outputs.logits[:, :-1, :]\n",
    "            logits_r = ref.logits[:, :-1, :]\n",
    "            labels = inputs.get('labels', inputs['input_ids'])[:, 1:]\n",
    "            if self.kl_limit_seq > 0:\n",
    "                t = min(self.kl_limit_seq, logits_s.shape[1])\n",
    "                logits_s = logits_s[:, -t:, :]\n",
    "                logits_r = logits_r[:, -t:, :]\n",
    "                labels   = labels[:, -t:]\n",
    "            mask = (labels != -100).float()\n",
    "            logp_s = torch.log_softmax(logits_s, dim=-1)\n",
    "            logp_r = torch.log_softmax(logits_r, dim=-1)\n",
    "            p_s    = torch.exp(logp_s)\n",
    "            kl_tok = (p_s * (logp_s - logp_r)).sum(-1) * mask\n",
    "            denom  = mask.sum().clamp_min(1.0)\n",
    "            kl_mean = kl_tok.sum() / denom\n",
    "            loss = loss + coef * kl_mean\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "tok = load_tokenizer()\n",
    "base = load_base_model()\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_cfg = LoraConfig(r=16, lora_alpha=16, lora_dropout=0.05, bias='none', task_type='CAUSAL_LM', target_modules=['q_proj','v_proj'])\n",
    "student = get_peft_model(base, lora_cfg)\n",
    "student.gradient_checkpointing_enable()\n",
    "student.enable_input_require_grads()\n",
    "student.config.use_cache = False\n",
    "ref_model = load_base_model().eval()\n",
    "for p in ref_model.parameters(): p.requires_grad_(False)\n",
    "\n",
    "ds = load_dolly_as_sft(tok, max_samples=120)\n",
    "train_ds, val_ds = split_small(ds, test_size=0.1, seed=SEED)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    enc = tok(batch['text'], truncation=True, max_length=256)\n",
    "    # labels created by collator\n",
    "    return enc\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='out_kl_sft', per_device_train_batch_size=2, gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1, learning_rate=1e-4, fp16=False, logging_steps=5,\n",
    "    eval_steps=5,\n",
    "    eval_strategy='steps', save_strategy='no', report_to=[], max_steps=20,\n",
    ")\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "trainer = KLTrainer(model=student, args=args, train_dataset=train_tok, eval_dataset=val_tok, data_collator=collator, ref_model=ref_model, kl_coef=0.05, kl_warmup_steps=300, kl_limit_seq=128)\n",
    "pre = trainer.evaluate()\n",
    "pre_ppl = math.exp(pre['eval_loss']) if pre['eval_loss'] < 20 else float('inf')\n",
    "print('KL-SFT pre-train eval_loss:', pre['eval_loss'], 'ppl:', pre_ppl)\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "ppl = math.exp(metrics['eval_loss']) if metrics['eval_loss'] < 20 else float('inf')\n",
    "print('KL-SFT post-train eval_loss:', metrics['eval_loss'], 'ppl:', ppl)\n",
    "print('Eval losses over steps:', [(h.get('step'), h.get('eval_loss')) for h in trainer.state.log_history if 'eval_loss' in h])\n",
    "trainer.save_model('out_kl_sft')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d190d31",
   "metadata": {},
   "source": [
    "## 7) Inference — compare adapters\n",
    "Set which adapter to load: 'base' | 'soft_prompt' | 'lora_sft' | 'kl_sft'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a1377d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T10:27:37.351156Z",
     "iopub.status.busy": "2025-10-26T10:27:37.351156Z",
     "iopub.status.idle": "2025-10-26T10:27:42.960968Z",
     "shell.execute_reply": "2025-10-26T10:27:42.960968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\peft\\peft_model.py:2066: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a short friendly welcome message for a workshop:\n",
      "\n",
      "**Welcome to our workshop!**\n",
      "\n",
      "**Hi there! I'm excited to be here with you today to learn about the amazing world of plants and how to grow them.\n",
      "\n",
      "**Welcome to our workshop!**\n",
      "\n",
      "**Hi there! I'm here to help you learn about the amazing world of plants.\n",
      "\n",
      "**What is a plant?**\n",
      "\n",
      "A plant is a living organism that grows and grows in the ground or in water. It's made up of roots, stems, leaves, flowers, and seeds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ADAPTER = 'soft_prompt'\n",
    "from transformers import GenerationConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_for_inference(adapter: str):\n",
    "    tok = load_tokenizer()\n",
    "    model = load_base_model()\n",
    "    if adapter == 'soft_prompt':\n",
    "        model = PeftModel.from_pretrained(model, 'out_soft_prompt')\n",
    "    elif adapter == 'lora_sft':\n",
    "        model = PeftModel.from_pretrained(model, 'out_lora_sft')\n",
    "    elif adapter == 'kl_sft':\n",
    "        model = PeftModel.from_pretrained(model, 'out_kl_sft')\n",
    "    return tok, model\n",
    "\n",
    "tok, model = load_for_inference(ADAPTER)\n",
    "chat = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\",  \"content\":\"Write a short friendly welcome message for a workshop.\"},\n",
    "]\n",
    "text = tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tok(text, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "print(tok.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b460a3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Soft Prompt: val loss 2.9554 → 2.5475 (Δ -0.4079); steps: [(5, 2.712756395339966), (10, 2.620220899581909), (15, 2.559868097305298), (20, 2.547456979751587), (20, 2.547456979751587)]\n",
    "- LoRA SFT: val loss 2.8463 → 2.7827 (Δ -0.0636); steps: [(5, 2.8197438716888428), (10, 2.8000917434692383), (15, 2.7874844074249268), (20, 2.7826976776123047), (20, 2.7826976776123047)]\n",
    "- KL-SFT: val loss 2.8742 → 2.8099 (Δ -0.0644); steps: [(5, 2.847374200820923), (10, 2.8274734020233154), (15, 2.8147003650665283), (20, 2.8098623752593994), (20, 2.8098623752593994)]\n",
    "\n",
    "- Soft Prompt showed the largest drop under this tiny budget, which is plausible when style/format dominates.\n",
    "- KL-SFT trades a bit of task fit for staying closer to the base (anchoring), consistent with a slightly higher final loss vs plain LoRA.\n",
    "- If general abilities or safety regress after SFT, increase kl_coef or LR-warmup and keep eval curves trending down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
